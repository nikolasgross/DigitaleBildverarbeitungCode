{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documentation Neuronal Network\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# We will use this cell to import all the packages you will need in the following - think of it as turning on all your systems\n",
    "# in your cockpit\n",
    "\n",
    "# This makes sure that if you change code in your external scripts, they will be updated\n",
    "\n",
    "from IPython.display import display, clear_output\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from keras.api._v2.keras import layers\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from keras.regularizers import l2\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import datetime\n",
    "import umap\n",
    "\n",
    "cwd = os.getcwd ()\n",
    "\n",
    "# now go ahead and Run the cell. This might take a while...\n",
    "# while the cell is running, you will see ln[*] next to it. Once it finished, you will see the number of execution\n",
    "# In case you want to interrupt the run of a cell, press Ctrl + C (on your german keyboard, this is Strg + C) \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Defining the model and data parameters\n",
    "\n",
    "Now we define our parameters to use them later\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model / data parameters\n",
    "num_classes = 10\n",
    "input_shape = (28, 28, 1)\n",
    "keep_index = [39306, 45926, 35779, 9059, 3639, 29049, 15459, 26151, 56284, 1733]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the MNIST dataset\n",
    "\n",
    "Before any data science or machine learning project, it is essential to get to know your data. This will enable you to detect issues, noise, pitfalls and understand what your model will learn at the end of the day. With the MNIST dataset, you will be working with a beautiful, cleaned, easy to understand, low-memory, large-scale dataset. \n",
    "Full disclaimer, MNIST is excellent for learning and research purposes, yet this is not what you can expect in real-life. \n",
    "\n",
    "This initially becomes obvious as MNIST is so commonly used that the Keras packages got our back with loading the data in one line. We are loading pairs of samples and ground truth annotations for both the training set (for training our model) and the test set (for testing our model). \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train: (60000, 28, 28)\n",
      "y_train: (60000,)\n",
      "x_test: (10000, 28, 28)\n",
      "y_test: (10000,)\n"
     ]
    }
   ],
   "source": [
    "# Loading the MNIST dataset in one line\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Printing the shape\n",
    "print('x_train:', x_train.shape)\n",
    "print('y_train:', y_train.shape)\n",
    "print('x_test:', x_test.shape)\n",
    "print('y_test:', y_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining our function to calculate the mse of two pictures\n",
    "\n",
    "With this function we want to calculate the mse of two labels to get the most similar ones\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mse(imageA, imageB):\n",
    "\t# the 'Mean Squared Error' between the two images is the\n",
    "\t# sum of the squared difference between the two images;\n",
    "\t# NOTE: the two images must have the same dimension\n",
    "\terr = np.sum((imageA.astype(\"float\") - imageB.astype(\"float\")) ** 2)\n",
    "\terr /= float(imageA.shape[0] * imageA.shape[1])\n",
    "\t\n",
    "\t# return the MSE, the lower the error, the more \"similar\"\n",
    "\t# the two images are\n",
    "\treturn err\n",
    "\n",
    "x_train = x_train[0:10000]\n",
    "y_train = y_train[0:10000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train: (43, 28, 28)\n",
      "y_train: (43,)\n",
      "x_test: (10000, 28, 28)\n",
      "y_test: (10000,)\n"
     ]
    }
   ],
   "source": [
    "def search_for_most_similar_input(list, index):\n",
    "  store_mse = 1.0\n",
    "  remember_index = 1\n",
    "  for i in range (index, len(list)):\n",
    "    if i != index: \n",
    "      actual_mse = calculate_mse(list[index], list[i])\n",
    "      if actual_mse < store_mse:\n",
    "        store_mse = actual_mse\n",
    "        remember_index = i\n",
    "  return remember_index\n",
    "for calc in range (0,8):\n",
    "  for i in range (0,10):\n",
    "    filter_for_number = np.where(y_train == i)[0]\n",
    "    for y in range (len(x_train[filter_for_number])//2):\n",
    "      index = search_for_most_similar_input(x_train[filter_for_number], y)\n",
    "      x_train = np.delete(x_train, filter_for_number[index], 0)\n",
    "      y_train = np.delete(y_train,filter_for_number[index])\n",
    "      filter_for_number = np.where(y_train == i )[0]\n",
    "\n",
    "print('x_train:', x_train.shape)\n",
    "print('y_train:', y_train.shape)\n",
    "print('x_test:', x_test.shape)\n",
    "print('y_test:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_numbers=[]\n",
    "mse_numbers = []\n",
    "def searchBestImage():\n",
    "  for number in range (0,10):\n",
    "    mse_number = 0.0\n",
    "    filter_for_number = np.where(y_train == number)[0]\n",
    "    for number1 in range (len(filter_for_number)):\n",
    "      for number2 in range (len(filter_for_number)):\n",
    "        mse_number += calculate_mse(x_train[filter_for_number[number1]],x_train[filter_for_number[number2]])\n",
    "      mse_number = mse_number / len(filter_for_number)\n",
    "      mse_numbers.append(mse_number)\n",
    "    min = np.argmin(mse_numbers)\n",
    "    keep_index.append(filter_for_number[min])\n",
    "    mse_numbers.clear()\n",
    "    filter_for_number = np.where(y_train == number )[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "            featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "            samplewise_center=False,  # set each sample mean to 0\n",
    "            featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "            samplewise_std_normalization=False,  # divide each input by its std\n",
    "            zca_whitening=False,  # apply ZCA whitening\n",
    "            rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "            zoom_range = 0.1, # Randomly zoom image \n",
    "            width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "            height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "            horizontal_flip=False,  # randomly flip images\n",
    "            vertical_flip=False)  # randomly flip images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape:  (43, 28, 28, 1)\n",
      "y_train shape:  (43,)\n",
      "x_test shape (10000, 28, 28, 1)\n",
      "y_test shape (10000,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#searchBestImage()\n",
    "\n",
    "#x_train = x_train[keep_index]\n",
    "#y_train = y_train[keep_index]\n",
    "\n",
    "# Scale images to the [0, 1] range\n",
    "x_train = x_train.astype(\"float32\") / 255\n",
    "x_test = x_test.astype(\"float32\") / 255\n",
    "# Make sure images have shape (28, 28, 1)\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "print(\"x_train shape: \", x_train.shape)\n",
    "print(\"y_train shape: \", y_train.shape)\n",
    "print(\"x_test shape\", x_test.shape)\n",
    "print(\"y_test shape\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "marvin = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28,28)),\n",
    "  tf.keras.layers.Dense(256, activation='sigmoid',kernel_initializer='glorot_uniform'),\n",
    "  tf.keras.layers.Dense(128, activation='sigmoid',kernel_initializer='glorot_uniform'),\n",
    "  tf.keras.layers.Dropout(0.1),#\n",
    "  tf.keras.layers.Dense(9, activation='softmax',kernel_initializer='glorot_uniform', use_bias=True)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train: [0 1 2 3 4 6 7 8 9]\n",
      "x_red: (9, 28, 28)\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_2 (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 256)               200960    \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 9)                 1161      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 235017 (918.04 KB)\n",
      "Trainable params: 235017 (918.04 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/Users/nikolasgross/Desktop/venv/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/Users/nikolasgross/Desktop/venv/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/nikolasgross/Desktop/venv/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/Users/nikolasgross/Desktop/venv/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1151, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/Users/nikolasgross/Desktop/venv/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1209, in compute_loss\n        return self.compiled_loss(\n    File \"/Users/nikolasgross/Desktop/venv/lib/python3.11/site-packages/keras/src/engine/compile_utils.py\", line 277, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/Users/nikolasgross/Desktop/venv/lib/python3.11/site-packages/keras/src/losses.py\", line 143, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/Users/nikolasgross/Desktop/venv/lib/python3.11/site-packages/keras/src/losses.py\", line 270, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/Users/nikolasgross/Desktop/venv/lib/python3.11/site-packages/keras/src/losses.py\", line 2221, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"/Users/nikolasgross/Desktop/venv/lib/python3.11/site-packages/keras/src/backend.py\", line 5573, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 1) and (None, 9) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 27\u001b[0m\n\u001b[1;32m     22\u001b[0m reduce_lr \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mReduceLROnPlateau (monitor \u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m, factor \u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m , min_delta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.00005\u001b[39m , \n\u001b[1;32m     23\u001b[0m                                                   patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m, min_lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     24\u001b[0m model_checkpoint \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mModelCheckpoint (filepath\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(cwd,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels/mnist_10_label.h5\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m     25\u001b[0m                                                        monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m,save_best_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 27\u001b[0m \u001b[43mmarvin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mmodel_checkpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43mreduce_lr\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m marvin_reloaded \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mload_model(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels/mnist_10_label.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     37\u001b[0m loss_and_metrics \u001b[38;5;241m=\u001b[39m marvin_reloaded\u001b[38;5;241m.\u001b[39mevaluate(x_test, y_test, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/venv/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/var/folders/f3/hj2y_l_56l16v7ms6lt0hs7c0000gn/T/__autograph_generated_filebgo0uxte.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/Users/nikolasgross/Desktop/venv/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/Users/nikolasgross/Desktop/venv/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/nikolasgross/Desktop/venv/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/Users/nikolasgross/Desktop/venv/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1151, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/Users/nikolasgross/Desktop/venv/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1209, in compute_loss\n        return self.compiled_loss(\n    File \"/Users/nikolasgross/Desktop/venv/lib/python3.11/site-packages/keras/src/engine/compile_utils.py\", line 277, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/Users/nikolasgross/Desktop/venv/lib/python3.11/site-packages/keras/src/losses.py\", line 143, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/Users/nikolasgross/Desktop/venv/lib/python3.11/site-packages/keras/src/losses.py\", line 270, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/Users/nikolasgross/Desktop/venv/lib/python3.11/site-packages/keras/src/losses.py\", line 2221, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"/Users/nikolasgross/Desktop/venv/lib/python3.11/site-packages/keras/src/backend.py\", line 5573, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 1) and (None, 9) are incompatible\n"
     ]
    }
   ],
   "source": [
    "batch_size = 8\n",
    "epochs = 150\n",
    "\n",
    "#print(\"Number of input:\",lda.n_components_)\n",
    "keep_index = [39306, 45926, 35779, 9059, 3639, 15459, 26151, 56284, 1733]\n",
    "\n",
    "x_train=x_train[keep_index]\n",
    "y_train=y_train[keep_index]\n",
    "print(\"y_train:\",y_train)\n",
    "print('x_red:', x_train.shape)\n",
    "\n",
    "marvin.compile(loss=\"categorical_crossentropy\", optimizer=keras.optimizers.legacy.Adam(learning_rate=0.01), metrics=[\"accuracy\"])\n",
    "\n",
    "marvin.summary()\n",
    "\n",
    "#%%\n",
    "logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
    "\n",
    "#%%\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping (monitor='val_accuracy',patience=15,mode=\"max\") \n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau (monitor ='val_accuracy', factor =0.5 , min_delta=0.00005 , \n",
    "                                                  patience=6, min_lr=0.001, mode=\"max\")\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint (filepath=os.path.join(cwd,'models/mnist_10_label.h5'),\n",
    "                                                       monitor='val_accuracy',save_best_only=True, mode=\"max\")\n",
    "\n",
    "marvin.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs= 200,\n",
    "    batch_size= 32,\n",
    "    validation_data=(x_test, y_test),\n",
    "    callbacks=[model_checkpoint,reduce_lr]\n",
    ")\n",
    "\n",
    "marvin_reloaded = tf.keras.models.load_model('models/mnist_10_label.h5')\n",
    "loss_and_metrics = marvin_reloaded.evaluate(x_test, y_test, verbose=2)\n",
    "\n",
    "print(\"Test Loss\", loss_and_metrics[0])\n",
    "print(\"Test Accuracy\", loss_and_metrics[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let Marvin predict on the test set, so we have some data to evaluate his performance.\n",
    "predictions = marvin_reloaded.predict([x_test])\n",
    "\n",
    "# Remember that the prediction of Marvin is a probability distribution over all ten-digit classes\n",
    "# We want him to assign the digit class with the highest probability to the sample.\n",
    "predictions = np.argmax(predictions, axis=1)\n",
    "#pd.DataFrame(predictions)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
